{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import  nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 196\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "class ShallowFCNet(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(ShallowFCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMAGE_SIZE, 120)\n",
    "        self.fc2 = nn.Linear(120, NUM_CLASSES)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"ShallowFCNet, dropout = {dropout}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, IMAGE_SIZE)))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DeepFCNet(nn.Module):\n",
    "    def __init__(self, nb_layers=4, dropout = 0):\n",
    "        super(DeepFCNet, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.name = f\"DeepFCNet({nb_layers})\"\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        acc = IMAGE_SIZE\n",
    "        if nb_layers % 2 !=0:\n",
    "            nb_layers = nb_layers - 1\n",
    "        for l in range(nb_layers):\n",
    "            if l < nb_layers/2:\n",
    "                self.layers.append(nn.Linear(acc, acc*2))\n",
    "                acc = acc*2\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(acc, int(acc/2)))\n",
    "                acc = int(acc/2)\n",
    "        self.layers.append(nn.Linear(IMAGE_SIZE, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acc = IMAGE_SIZE\n",
    "        for l in range(len(self.layers)-1):\n",
    "            x = F.relu(self.layers[l](x.view(-1, acc)))\n",
    "            x = self.drop(x)\n",
    "            if l < (len(self.layers)-1)/2:\n",
    "                acc = acc*2\n",
    "            else:\n",
    "                acc = int(acc/2)\n",
    "        x = self.layers[len(self.layers)-1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=4)\n",
    "        self.fc1 = nn.Linear(32*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"BasicCNN(dropout = {dropout})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x.view(-1, 1, 14, 14)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 32*5*5)))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class BasicCNN_bn(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(BasicCNN_bn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 32, kernel_size=4)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=4)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"BasicCNN with batch normalization, dropout = {dropout}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x.view(-1, 1, 14, 14))))\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*5*5)))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LeNet4(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(LeNet4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, padding = 9)\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"LeNet4, dropout = {dropout}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.view(-1, 1, 14, 14))\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.fc1(x.view(-1, 400)))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding = 9)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"LeNet45, dropout = {dropout}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.view(-1, 1, 14, 14))\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.fc1(x.view(-1, 400)))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv2 = nn.Conv2d(6, 32, kernel_size=4)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=4)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.conv2_bis = nn.Conv2d(6, 64, kernel_size=1)\n",
    "        self.avg = nn.AvgPool2d(kernel_size = 2)\n",
    "        self.max = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv2_bis_bn = nn.BatchNorm2d(64)\n",
    "    def forward(self, x):\n",
    "        y = self.conv2_bn(self.conv2(x))\n",
    "        y = F.relu(y)\n",
    "        y = self.conv3_bn(self.conv3(y))\n",
    "        y += F.relu(self.conv2_bis_bn(self.avg(self.conv2_bis(x)))) + F.relu(self.conv2_bis_bn(self.max(self.conv2_bis(x))))\n",
    "        y = F.relu(y)\n",
    "        return y\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, dropout = 0):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "        self.resblock = ResBlock(dropout)\n",
    "        self.fc1 = nn.Linear(64*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.name = f\"Residual network inspired from BasicCNN_bn, dropout = {dropout}\"\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x.view(-1, 1, 14, 14))))\n",
    "        x = self.resblock(x)\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*5*5)))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "optimizer_methods = {\n",
    "    'SGD': (lambda parameters, eta, momentum: optim.SGD(parameters(), eta, momentum = momentum)),\n",
    "    'Adam': (lambda parameters, eta, momentum: optim.Adam(parameters(), eta))\n",
    "}\n",
    "\n",
    "def train_model(model, train, train_classes, test, test_classes,\n",
    "                mini_batch_size, eta, criterion, nb_epochs, momentum, optimizer_name):\n",
    "\n",
    "    train_accuracy = torch.zeros(nb_epochs)\n",
    "    test_accuracy = torch.zeros(nb_epochs)\n",
    "    train_loss = torch.zeros(nb_epochs)\n",
    "    test_loss = torch.zeros(nb_epochs)\n",
    "    N_train = train.size(0)\n",
    "    N_test = test.size(0)\n",
    "\n",
    "    optimizer = optimizer_methods[optimizer_name](model.parameters, eta, momentum)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        correct_train_digits = 0\n",
    "        for batch in range(0, N_train, mini_batch_size):\n",
    "            output = model(train.narrow(0, batch, mini_batch_size))\n",
    "            _, predicted_classes = output.max(1)\n",
    "            loss = criterion(output, train_classes.narrow(0, batch, mini_batch_size))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            correct_train_digits += (train_classes[batch:batch+mini_batch_size] == predicted_classes).sum().item()\n",
    "\n",
    "        train_loss[epoch] = loss.item()\n",
    "        with torch.no_grad():\n",
    "            output = model(test)\n",
    "            loss = criterion(output, test_classes)\n",
    "            test_loss[epoch] = loss.item()\n",
    "            _, predicted_classes = output.max(1)\n",
    "            correct_test_digits = (test_classes == predicted_classes).sum().item()\n",
    "\n",
    "\n",
    "        # compute accuracy\n",
    "        train_accuracy[epoch] = correct_train_digits / N_train\n",
    "        test_accuracy[epoch] = correct_test_digits / N_test\n",
    "\n",
    "    return train_accuracy, test_accuracy, train_loss, test_loss\n",
    "\n",
    "\n",
    "def compute_project_accuracy(model, input1, input2, target):\n",
    "    output1 = model(input1)\n",
    "    output2 = model(input2)\n",
    "    _, predicted_classes1 = output1.max(1)\n",
    "    _, predicted_classes2 = output2.max(1)\n",
    "\n",
    "    nb_correct_project = (target == (predicted_classes1 <= predicted_classes2)).sum().item()\n",
    "\n",
    "\n",
    "    return float(nb_correct_project / target.size(0))\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def train_test(model, train, test, train_classes, test_classes,\n",
    "            train_target, test_target, mini_batch_size, criterion,\n",
    "             nb_epochs, eta = 1e-2, momentum = 0.9, optimizer_name = 'SGD', repeats = 25):\n",
    "    all_results = []\n",
    "\n",
    "    N =  int(len(train)/2)\n",
    "    train_comparison = torch.zeros(repeats,1)\n",
    "    test_comparison  = torch.zeros(repeats,1)\n",
    "\n",
    "    train_loss = torch.zeros(repeats, nb_epochs)\n",
    "    test_loss = torch.zeros(repeats, nb_epochs)\n",
    "\n",
    "    train_acc = torch.zeros(repeats, nb_epochs)\n",
    "    test_acc = torch.zeros(repeats, nb_epochs)\n",
    "\n",
    "\n",
    "    for i in range(repeats):\n",
    "        model.apply(weights_init)\n",
    "\n",
    "        train_acc[i], test_acc[i], train_loss[i], test_loss[i] = train_model(model, train, train_classes,\n",
    "            test, test_classes, mini_batch_size, eta, criterion, nb_epochs, momentum,\n",
    "            optimizer_name)\n",
    "\n",
    "        # plot_accuracy(train_comparison[i], test_comparison[i], nb_epochs)\n",
    "\n",
    "        train_comparison[i] = compute_project_accuracy(model, train[: N], train[N: ], train_target)\n",
    "        test_comparison[i] = compute_project_accuracy(model, test[: N], test[N: ], test_target)\n",
    "\n",
    "    all_results.append({\"Model\": model.name, \"Optimizer\": optimizer_name , \"Epochs\": nb_epochs, \"Eta\": eta, \"Train Accuracy Mean\": train_comparison.mean().item(),\"Test Accuracy Mean\": test_comparison.mean().item(), \"Train Accuracy Std\":  train_acc.std().item(), \"Test Accuracy Std\": test_acc.std().item(), \"Digit acc table\":     train_acc.mean(axis= 0)\n",
    "            , \"test Digit Accuracy Table\":     test_acc.mean(axis= 0)})\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Module(object):\n",
    "\n",
    "    def forward (self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward (self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param (self):\n",
    "        return []\n",
    "\n",
    "'''\n",
    "Applies a linear transformation to the incoming data: y = xA^T + b\n",
    "ARGS:\n",
    "    - weight: the learnable weights\n",
    "    - bias: the learnable bias\n",
    "SHAPE:\n",
    "    - weight: (out_features, in_features)\n",
    "    - bias: (out_features)\n",
    "\n",
    "'''\n",
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        init_bound = 1/math.sqrt(in_features)\n",
    "        self.weight = torch.Tensor(out_features, in_features).uniform_(-init_bound, init_bound)\n",
    "        if bias:\n",
    "            self.bias = torch.Tensor(out_features).uniform_(-init_bound, init_bound)\n",
    "        else:\n",
    "            self.bias = torch.zeros(out_features)\n",
    "        self.grad_weight = torch.zeros(self.weight.size())\n",
    "        self.grad_bias = torch.zeros(self.bias.size())\n",
    "\n",
    "    def forward (self, input):\n",
    "        self.previous_layer = input\n",
    "        return input.mm(self.weight.T) + self.bias\n",
    "\n",
    "    def backward (self, gradwrtoutput):\n",
    "        self.grad_weight.add_(gradwrtoutput.T.mm(self.previous_layer))\n",
    "        self.grad_bias.add_(gradwrtoutput.sum(0))\n",
    "        grad_input = gradwrtoutput.mm(self.weight)\n",
    "        return grad_input\n",
    "\n",
    "    def param (self):\n",
    "        return [(self.weight, self.grad_weight), (self.bias, self.grad_bias)]\n",
    "\n",
    "class ReLU(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def forward (self, input):\n",
    "        self.input = input\n",
    "        zeroes = torch.full(input.size(), 0.0, dtype = float )\n",
    "        return torch.where(input > 0, input.float(), zeroes.float())\n",
    "    def backward (self, gradwrtoutput):\n",
    "        ones = torch.ones(gradwrtoutput.size())\n",
    "        zeroes = torch.full(gradwrtoutput.size(), 0.0, dtype = float )\n",
    "        derivative = torch.where(self.input > 0, ones.float(), zeroes.float())\n",
    "        return gradwrtoutput * derivative\n",
    "    def param (self):\n",
    "        return [(None, None)]\n",
    "\n",
    "class Tanh(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    def forward (self, input):\n",
    "        self.input = input\n",
    "        return input.tanh()\n",
    "    def backward (self, gradwrtoutput):\n",
    "        derivative = 1 - (self.input.tanh()).pow(2)\n",
    "        return gradwrtoutput * derivative\n",
    "    def param (self):\n",
    "        return [(None, None)]\n",
    "\n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        self.modules = list(modules)\n",
    "\n",
    "    def forward (self, input):\n",
    "        x = input\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward (self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "\n",
    "    def param (self):\n",
    "        parameters = []\n",
    "        for module in self.modules:\n",
    "            parameters.append(module.param())\n",
    "        return parameters\n",
    "\n",
    "class SGD():\n",
    "\n",
    "    def __init__(self, parameters, eta):\n",
    "        self.parameters = parameters\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(eta))\n",
    "        else:\n",
    "            self.eta = eta\n",
    "\n",
    "    def step(self):\n",
    "        for module in self.parameters:\n",
    "            for param, grad_param in module:\n",
    "                if (param is not None and grad_param is not None):\n",
    "                    param.sub_(self.eta * grad_param)\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for module in self.parameters:\n",
    "            for param, grad_param in module:\n",
    "                if (param is not None and grad_param is not None):\n",
    "                    grad_param.zero_()\n",
    "\n",
    "class LossMSE():\n",
    "    def loss(self, prediction, target):\n",
    "        return (prediction - target).pow(2).sum()\n",
    "\n",
    "    def grad(self, prediction, target):\n",
    "        return 2*(prediction - target)\n",
    "\n",
    "def normalize_data(x):\n",
    "    mean, std =  x.mean(), x.std()\n",
    "    x.sub_(mean).div_(std)\n",
    "\n",
    "def train_model(model, train, train_target, test, test_target,\n",
    "                mini_batch_size, eta, nb_epochs, normalize = True):\n",
    "\n",
    "    train_accuracy = torch.zeros(nb_epochs)\n",
    "    test_accuracy = torch.zeros(nb_epochs)\n",
    "    train_loss = torch.zeros(nb_epochs)\n",
    "    test_loss = torch.zeros(nb_epochs)\n",
    "    N_train = train.size(0)\n",
    "    N_test = test.size(0)\n",
    "\n",
    "    if normalize:\n",
    "        normalize_data(train)\n",
    "        normalize_data(test)\n",
    "\n",
    "\n",
    "    optimizer = SGD(model.param(), eta)\n",
    "    MSE = LossMSE()\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        nb_correct_classes_tr = 0\n",
    "        loss = 0\n",
    "\n",
    "        for batch in range(0, N_train, mini_batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(train.narrow(0, batch, mini_batch_size))\n",
    "            loss += MSE.loss(output, train_target.narrow(0, batch, mini_batch_size))\n",
    "            loss_grad = MSE.grad(output, train_target.narrow(0, batch, mini_batch_size))\n",
    "            _, predicted_classes = output.max(1)\n",
    "            model.backward(loss_grad)\n",
    "            optimizer.step()\n",
    "            nb_correct_classes_tr += (train_target.narrow(0, batch, mini_batch_size).argmax(1) == predicted_classes).sum().item()\n",
    "\n",
    "        train_loss[epoch] = loss\n",
    "        # compute test loss and accuracy without computing the gradients\n",
    "        output = model.forward(test)\n",
    "        loss = MSE.loss(output, test_target)\n",
    "        test_loss[epoch] = loss.item()\n",
    "        _, predicted_classes = output.max(1)\n",
    "        nb_correct_classes_te = (test_target.argmax(1) == predicted_classes).sum().item()\n",
    "\n",
    "\n",
    "        # compute accuracy\n",
    "        train_accuracy[epoch] = nb_correct_classes_tr / N_train\n",
    "        test_accuracy[epoch] = nb_correct_classes_te / N_test\n",
    "\n",
    "    return train_accuracy, test_accuracy, train_loss, test_loss\n",
    "\n",
    "\n",
    "def generate_data(size):\n",
    "    input = torch.Tensor(size, 2).uniform_(0, 1)\n",
    "    target = input.sub(torch.tensor([0.5, 0.5])).pow(2).sum(1).sub(1 / (2*math.pi)).sign().add(1).div(2).long()\n",
    "    return input, target\n",
    "\n",
    "def one_hot_encoding(target):\n",
    "    onehot = torch.zeros(target.size(0), 2).fill_(0)\n",
    "    onehot[range(onehot.shape[0]), target]=1\n",
    "    return onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prologue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c6e7f334616c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprologue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_pair_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prologue' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(1000)\n",
    "\n",
    "train = torch.cat(( train_input[:, 0, :, :], train_input[:, 1, :, :]), 0)\n",
    "test = torch.cat((test_input[:, 0, :, :], test_input[:, 1, :, :]), 0)\n",
    "\n",
    "train_classes = torch.cat((train_classes[:, 0], train_classes[:, 1]), 0)\n",
    "test_classes = torch.cat((test_classes[:, 0], test_classes[:, 1]), 0)\n",
    "\n",
    "models = [ShallowFCNet(), DeepFCNet(), BasicCNN(), BasicCNN_bn(), LeNet4(), LeNet5(), ResNet()]\n",
    "optimizers = ['SGD']\n",
    "dropouts = [0, 0.25]\n",
    "criterions = [nn.CrossEntropyLoss(), nn.MultiMarginLoss()]\n",
    "epochs = [1]\n",
    "\n",
    "all_results = []\n",
    "PATH =  os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "for model in models:\n",
    "    for optimizer in optimizers:\n",
    "        for criterion in criterions:\n",
    "            for dropout in dropouts:\n",
    "                for epoch in epochs:\n",
    "                    all_results = train_test(model, train, test, train_classes,\n",
    "                                test_classes, train_target, test_target, 100,\n",
    "                                criterion, epoch, optimizer_name = optimizer)\n",
    "                    path = \"/models/\" + model.name + \"_\" + optimizer + \"_\" + str(dropout) + \"_\" + str(epoch)\n",
    "                    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with open('comparison_models.json', 'w') as json_file:\n",
    "    json.dump(all_results[0], json_file)\n",
    "print(all_results)\n",
    "\n",
    "print('Project 1 done')\n",
    "print('')\n",
    "\n",
    "\n",
    "train, train_target = generate_data(1000)\n",
    "test, test_target = generate_data(1000)\n",
    "\n",
    "train_one_hot_target = one_hot_encoding(train_target)\n",
    "test_one_hot_target = one_hot_encoding(test_target)\n",
    "\n",
    "# Requirements given by project2\n",
    "input_units = 2\n",
    "output_units = 2\n",
    "nb_hidden_units = 25\n",
    "\n",
    "model = Sequential(Linear(input_units, nb_hidden_units), Tanh(),\n",
    "                         Linear(nb_hidden_units, nb_hidden_units), Tanh(),\n",
    "                         Linear(nb_hidden_units, nb_hidden_units), Tanh(),\n",
    "                         Linear(nb_hidden_units, output_units))\n",
    "\n",
    "train_model(model, train, train_one_hot_target, test, test_one_hot_target, 25, 1e-4, 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
