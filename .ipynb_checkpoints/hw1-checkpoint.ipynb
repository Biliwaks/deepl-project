{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "import torch\n",
    "from torch import  nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 196\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_1 = train_input[:, 0, :, :]\n",
    "train_input_2 = train_input[:, 1, :, :]\n",
    "test_input_1 = test_input[:, 0, :, :]\n",
    "test_input_2 = test_input[:, 1, :, :]\n",
    "train_classes_1 = train_classes[:, 0]\n",
    "train_classes_2 = train_classes[:, 1]\n",
    "test_classes_1 = test_classes[:, 0]\n",
    "test_classes_2 = test_classes[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFullyConncectedNet(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ShallowFullyConncectedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMAGE_SIZE, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, NUM_CLASSES)\n",
    "        self.name = f\"ShallowFullyConncectedNet({nb_hidden})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, IMAGE_SIZE)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFullyConncectedNet(nn.Module):\n",
    "    def __init__(self, nb_layers):\n",
    "        super(DeepFullyConncectedNet, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.name = f\"DeepFullyConncectedNet({nb_layers})\"\n",
    "        acc = IMAGE_SIZE\n",
    "        if nb_layers % 2 !=0:\n",
    "            nb_layers = nb_layers - 1\n",
    "        for l in range(nb_layers):\n",
    "            if l < nb_layers/2:\n",
    "                self.layers.append(nn.Linear(acc, acc*2))\n",
    "                acc = acc*2\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(acc, int(acc/2)))\n",
    "                acc = int(acc/2)\n",
    "        self.layers.append(nn.Linear(IMAGE_SIZE, 10))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        acc = IMAGE_SIZE\n",
    "        for l in range(len(self.layers)-1):\n",
    "            x = F.relu(self.layers[l](x.view(-1, acc)))\n",
    "            if l < (len(self.layers)-1)/2:\n",
    "                acc = acc*2\n",
    "            else:\n",
    "                acc = int(acc/2)\n",
    "        x = self.layers[len(self.layers)-1](x)\n",
    "\n",
    "       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# class BasicCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net2, self).__init__()\n",
    "#         nb_hidden = 200\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "#         self.fc1 = nn.Linear(9 * 64, nb_hidden)\n",
    "#         self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.fc1(x.view(-1, 9 * 64)))\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# class ResNetBlock(nn.Module):\n",
    "#     def __init__(self, nb_channels, kernel_size,\n",
    "#                  skip_connections = True, batch_normalization = True):\n",
    "#         super(ResNetBlock, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(nb_channels, nb_channels,\n",
    "#                                kernel_size = kernel_size,\n",
    "#                                padding = (kernel_size - 1) // 2)\n",
    "\n",
    "#         self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(nb_channels, nb_channels,\n",
    "#                                kernel_size = kernel_size,\n",
    "#                                padding = (kernel_size - 1) // 2)\n",
    "\n",
    "#         self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "#         self.skip_connections = skip_connections\n",
    "#         self.batch_normalization = batch_normalization\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         if self.batch_normalization: y = self.bn1(y)\n",
    "#         y = F.relu(y)\n",
    "#         y = self.conv2(y)\n",
    "#         if self.batch_normalization: y = self.bn2(y)\n",
    "#         if self.skip_connections: y = y + x\n",
    "#         y = F.relu(y)\n",
    "\n",
    "#         return y\n",
    "    \n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "\n",
    "#     def __init__(self, nb_residual_blocks, nb_channels,\n",
    "#                  kernel_size = 3, nb_classes = 10,\n",
    "#                  skip_connections = True, batch_normalization = True):\n",
    "#         super(ResNet, self).__init__()\n",
    "\n",
    "#         self.conv = nn.Conv2d(3, nb_channels,\n",
    "#                               kernel_size = kernel_size,\n",
    "#                               padding = (kernel_size - 1) // 2)\n",
    "#         self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "#         self.resnet_blocks = nn.Sequential(\n",
    "#             *(ResNetBlock(nb_channels, kernel_size, skip_connections, batch_normalization)\n",
    "#               for _ in range(nb_residual_blocks))\n",
    "#         )\n",
    "\n",
    "#         self.fc = nn.Linear(nb_channels, nb_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn(self.conv(x)))\n",
    "#         x = self.resnet_blocks(x)\n",
    "#         x = F.avg_pool2d(x, 32).view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, eta, criterion = nn.CrossEntropyLoss(), nb_epochs = 25):\n",
    "   \n",
    "    optimizer = optim.SGD(model.parameters(), eta)\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input1, input2, classes1, classes2, target, mini_batch_size):\n",
    "    nb_errors = 0\n",
    "    for b in range(0, input1.size(0), mini_batch_size):\n",
    "        output1 = model(input1.narrow(0, b, mini_batch_size))\n",
    "        output2 = model(input2.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes1 = output1.max(1)\n",
    "        _, predicted_classes2 = output2.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k] != (predicted_classes1[k] <= predicted_classes2[k]):\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, mini_batch_size, eta, criterion, nb_epochs):\n",
    "    train = torch.cat((train_input_1, train_input_2), 0)\n",
    "    target = torch.cat((train_classes_1, train_classes_2), 0)\n",
    "\n",
    "    train_model(model, train, target, mini_batch_size, eta, criterion, nb_epochs)\n",
    "    train_error = compute_nb_errors(model, train_input_1, train_input_2, train_classes_1, train_classes_2, train_target, mini_batch_size) / target.size(0) * 100\n",
    "    test_error = compute_nb_errors(model, test_input_1, test_input_2, test_classes_1, test_classes_2, test_target, mini_batch_size) / target.size(0) * 100\n",
    "    print(f'MODEL: {model.name}, BATCH_SIZE: { mini_batch_size}, CRITERION: {str(criterion)}, EPOCHS: {nb_epochs}, train_error:{train_error}%, test_error: {test_error}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ShallowFullyConncectedNet(1000), BATCH_SIZE: 100, CRITERION: CrossEntropyLoss(), EPOCHS: 25, train_error:22.5%, test_error: 23.400000000000002%\n"
     ]
    }
   ],
   "source": [
    "train_test(ShallowFullyConncectedNet(1000), mini_batch_size=100, eta = 1e-1, criterion =nn.CrossEntropyLoss(), nb_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: DeepFullyConncectedNet(7), BATCH_SIZE: 100, CRITERION: CrossEntropyLoss(), EPOCHS: 25, train_error:22.05%, test_error: 23.25%\n"
     ]
    }
   ],
   "source": [
    "train_test(DeepFullyConncectedNet(7), 100, eta = 1e-1, criterion =nn.CrossEntropyLoss(), nb_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
